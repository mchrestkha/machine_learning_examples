{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf8ec3e0-c954-48a8-95a5-0e5c2d320841",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b25bef4f-3262-4d94-879e-36a0c50eaf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import utils\n",
    "import mercury as mr\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', '{:.10f}'.format)\n",
    "from google.cloud.exceptions import NotFound\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "#Vertex AI libraries\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "from vertexai.preview.tuning import sft\n",
    "from vertexai.preview.generative_models import GenerativeModel, GenerationConfig, Part\n",
    "from vertexai.evaluation import EvalTask, MetricPromptTemplateExamples, PointwiseMetric, PairwiseMetric\n",
    "\n",
    "#OpenAI library\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4899ea-e1c1-4802-84f2-c5143b32f837",
   "metadata": {},
   "source": [
    "## Loading Dataset From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d12921cc-be7f-49e2-8383-d5d43dd4dda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5377f9ce-2948-41e9-9d29-e05cd632e023",
   "metadata": {},
   "source": [
    "## Creating different sized tuning Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f608e323-3600-47c9-8db2-2e3f5216ebdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full size datasets\n",
    "train12460=dataset[\"train\"].to_list()\n",
    "valid500 =dataset[\"validation\"].to_list()\n",
    "test1500 =dataset[\"test\"].to_list()\n",
    "\n",
    "base_instruction=\"Summarize the following dialogue: \"\n",
    "for item in test1500: \n",
    "    item[\"dialogue\"] = base_instruction + item[\"dialogue\"]\n",
    "\n",
    "# smaller datasets for rapid testing\n",
    "train2000=train12460[:2000]\n",
    "test100=test1500[:100]\n",
    "test250=test1500[:250]\n",
    "test10=test1500[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda92d07-c449-4316-ad64-6c746ba33367",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Formatting for Tuning API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4466d7e-e982-4281-bb75-c9aa9b774091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare data for Gemini 1.5 Tuning\n",
    "# Define a base prompt for zero-shot summarization \n",
    "base_instruction=\"Summarize the following dialogue: \"\n",
    "utils.format_tuning_dataset(train2000, valid500, base_instruction, \"dialogsum_train2000_inst\",\"dialogsum_valid500_inst\")\n",
    "utils.format_tuning_dataset(train12460, valid500, base_instruction, \"dialogsum_train12460_inst\",\"dialogsum_valid500_inst\")\n",
    "\n",
    "base_instruction=\"\"\n",
    "utils.format_tuning_dataset(train2000, valid500, base_instruction, \"dialogsum_train2000_no_inst\",\"dialogsum_valid500_no_inst\")\n",
    "utils.format_tuning_dataset(train12460, valid500, base_instruction, \"dialogsum_train12460_no_inst\",\"dialogsum_valid500_no_inst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837758af-e2c5-47b6-b105-9453beeacd2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.delete_and_upload(\"dialogsum_train12460_inst.jsonl\")\n",
    "utils.delete_and_upload(\"dialogsum_train2000_inst.jsonl\")\n",
    "utils.delete_and_upload(\"dialogsum_valid500_inst.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871cb58-bf7f-49c2-8612-f48c9a0d8d15",
   "metadata": {},
   "source": [
    "## Submit Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debeed0e-a4f6-4956-94cf-148103819d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=\"gemini-1.5-flash-001\"\n",
    "utils.tune_gemini(\"gs://mchrestkha-sample-data/dialogsum/dialogsum_train2000_inst.jsonl\", \"gs://mchrestkha-sample-data/dialogsum/dialogsum_valid500_inst.jsonl\", model, \"dialogsum_2000_inst\")\n",
    "utils.tune_gemini(\"gs://mchrestkha-sample-data/dialogsum/dialogsum_train12460_inst.jsonl\", \"gs://mchrestkha-sample-data/dialogsum/dialogsum_valid500_inst.jsonl\", model, \"dialogsum_124600_inst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de09063-2454-41d0-9963-3fac8b79966f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OpenAI Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ebb59-e764-453f-bd76-59a2083ff81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare data for OpenAI  Tuning\n",
    "# Define a base prompt for zero-shot summarization \n",
    "system_prompt=\"Summarize the following dialogue: \"\n",
    "\n",
    "# Initialize lists to store messages for training and validation\n",
    "train_messages = []\n",
    "validation_messages = []\n",
    "train = train2000\n",
    "valid = valid500\n",
    "\n",
    "# Iterate over training data and create messages for each dialogue-summary pair\n",
    "for d in train:\n",
    "  prompts = []\n",
    "  prompts.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "  prompts.append({\"role\": \"user\", \"content\": d[\"dialogue\"]})\n",
    "  prompts.append({\"role\": \"assistant\", \"content\": d[\"summary\"]})\n",
    "  train_messages.append({'messages': prompts})\n",
    "\n",
    "# Iterate over validation data and create messages similarly\n",
    "for d in valid:\n",
    "  prompts = []\n",
    "  prompts.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "  prompts.append({\"role\": \"user\", \"content\": d[\"dialogue\"]})\n",
    "  prompts.append({\"role\": \"assistant\", \"content\": d[\"summary\"]})\n",
    "  validation_messages.append({'messages': prompts})\n",
    "\n",
    "    # Print lengths of message lists and an example training message\n",
    "len(train_messages), len(validation_messages), train_messages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2af86-efab-4389-ae6b-e9a31e21e5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save to JSON locally\n",
    "utils.dicts_to_jsonl(train_messages, \"openai_dialogsum_train2000\", False)\n",
    "utils.dicts_to_jsonl(validation_messages, \"openai_dialogsum_valid500\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac72e15-9afe-48ce-847e-255d26dfea90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register & Uplaod Files to OpenAI Storage\n",
    "client.files.create(\n",
    "  file=open(\"openai_dialogsum_train2000.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"openai_dialogsum_valid500.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c953a-df68-402a-8ecd-89da6d2d885c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Submit Tuning Job\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-KxJuvj5sQ3kLQoI7f6X8S9PE\", \n",
    "  validation_file=\"file-QGvOkG9PtiZJ7y0L1JmNbzDE\",\n",
    "  model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935f0a5-b915-4a9f-b2ec-da49e4c986f3",
   "metadata": {},
   "source": [
    "## Running Predictions on Test Data\n",
    "### For X test examples takes Y min to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2d98e745-253a-44b5-aef7-c8d763ee7f03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-3334ba2b-c19a-4af7-9ef9-603c98ba9252\" href=\"#view-view-vertex-resource-3334ba2b-c19a-4af7-9ef9-603c98ba9252\">\n",
       "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
       "          <span>View Tuning Job</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-3334ba2b-c19a-4af7-9ef9-603c98ba9252');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/2137747369456828416?project=642508009780');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/2137747369456828416?project=642508009780', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 250/250 [13:44<00:00,  3.30s/row]\n"
     ]
    }
   ],
   "source": [
    "gemini_text = []\n",
    "openai_text = []\n",
    "gemini_tuned_text = []\n",
    "openai_tuned_text = []\n",
    "\n",
    "tuning_job = sft.SupervisedTuningJob(\"projects/642508009780/locations/us-central1/tuningJobs/2137747369456828416\")\n",
    "tuned_model = GenerativeModel(tuning_job.tuned_model_endpoint_name)\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "client = OpenAI()\n",
    "#test=test10\n",
    "#test=test1500\n",
    "test=test250\n",
    "\n",
    "for row in tqdm(test, desc=\"Processing\", unit=\"row\"):\n",
    "    try:\n",
    "        gemini_response = model.generate_content(contents=row[\"dialogue\"])\n",
    "        gemini_text.append(gemini_response.text)\n",
    "    except (ValueError, AttributeError):  # Catch broader potential errors\n",
    "        gemini_text.append(\"Blocked\")\n",
    "        \n",
    "    try:\n",
    "        gemini_tuned_response = tuned_model.generate_content(contents=row[\"dialogue\"])\n",
    "        gemini_tuned_text.append(gemini_tuned_response.text)\n",
    "    except (ValueError, AttributeError):  # Catch broader potential errors\n",
    "        gemini_tuned_text.append(\"Blocked\")\n",
    "\n",
    "    try:\n",
    "        openai_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            messages=[{\"role\": \"user\", \"content\": row[\"dialogue\"]}]\n",
    "        )\n",
    "        openai_text.append(openai_response.choices[0].message.content)\n",
    "    except (ValueError, AttributeError): \n",
    "        openai_text.append(\"Blocked\")\n",
    "\n",
    "    try:\n",
    "        openai_tuned_response = client.chat.completions.create(\n",
    "            model=\"ft:gpt-4o-mini-2024-07-18:personal::A3WwHRrJ\",\n",
    "            messages=[{\"role\": \"user\", \"content\": row[\"dialogue\"]}]\n",
    "        )\n",
    "        openai_tuned_text.append(openai_tuned_response.choices[0].message.content)\n",
    "    except (ValueError, AttributeError): \n",
    "        openai_tuned_text.append(\"Blocked\")\n",
    "\n",
    "# Directly create the final DataFrame with responses included\n",
    "df_final = pd.DataFrame(test)\n",
    "df_final[\"gemini_response\"] = gemini_text\n",
    "df_final[\"openai_response\"] = openai_text\n",
    "df_final[\"gemini_tuned_response\"] = gemini_tuned_text\n",
    "df_final[\"openai_tuned_response\"] = openai_tuned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "303a0d29-1348-4f39-9a9e-6db0da569f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final[\"summary_response\"]=df_final[\"summary\"]\n",
    "df_test_predictions_final=df_final\n",
    "df_test_predictions_final.to_csv('df_test_predictions_final.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c15c342-214f-4c05-a19d-61af00f31089",
   "metadata": {},
   "source": [
    "## Running Computation & Model Pointwise Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e13fe0c0-faff-4734-8939-2b2b004e7f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a pointwise custom summarization quality metric \n",
    "pointwise_custom_summary_metric_prompt = \"\"\"\n",
    "# Instruction\n",
    "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models.\n",
    "We will provide you with the user input and an AI-generated response.\n",
    "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step-by-step explanations for your rating, and only choose ratings from the Rating Rubric. **Explicitly include the word count of the response as the first step in your explanation**, and ensure it aligns with the criteria.\n",
    "\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing summarization quality, which measures the overall ability to summarize text.  The context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n",
    "\n",
    "## Criteria\n",
    "Less than 50 words: The response contains less than 50 words.  Use the following formula to count the words in the response: `=COUNTA(SPLIT(response, \" \"))`\n",
    "Groundedness: The response contains information included only in the context. The response does not reference any outside information.\n",
    "Observer Perspective: The response is written from an observer perspective.\n",
    "\n",
    "## Rating Rubric\n",
    "5: (Very good). The summary is less than 50 words, is grounded and is written as an observer.\n",
    "4: (Good). The summary is less than 50 words and is grounded.  \n",
    "3: (Ok). The summary is more than 50 words but mostly grounded\n",
    "2: (Bad). The summary is more than 50 words and not grounded.\n",
    "1: (Very bad). The summary is more than 50 words and not grounded.\n",
    "\n",
    "## Evaluation Steps\n",
    "STEP 1: Assess the response in aspects of word count, groundedness, and observer perspective according to the criteria.  **Use the provided formula to determine the EXACT word count**\n",
    "STEP 2: Score based on the rubric.\n",
    "\n",
    "# User Inputs and AI-generated Response\n",
    "## User Inputs\n",
    "\n",
    "### Prompt\n",
    "{prompt}\n",
    "\n",
    "## AI-generated Response\n",
    "{response}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pointwise_custom_summary_metric = PointwiseMetric(\n",
    "  metric=\"custom_point_summary_metric\",\n",
    "  metric_prompt_template=pointwise_custom_summary_metric_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c2afb2-b127-4cb7-86f6-b90d402bb5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 750 Vertex online evaluation service requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [50:03<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 750 metric requests are successfully computed.\n",
      "Evaluation Took:3003.383097610007 seconds\n",
      "Computing metrics with a total of 750 Vertex online evaluation service requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [50:00<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 750 metric requests are successfully computed.\n",
      "Evaluation Took:3000.8083779989975 seconds\n",
      "Computing metrics with a total of 750 Vertex online evaluation service requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [50:01<00:00,  4.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 750 metric requests are successfully computed.\n",
      "Evaluation Took:3001.101887780009 seconds\n",
      "Computing metrics with a total of 750 Vertex online evaluation service requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 483/750 [32:12<19:18,  4.34s/it] "
     ]
    }
   ],
   "source": [
    "def run_eval(dataset, col_prompt,col_response,col_reference):\n",
    "    eval_dataset_comp=dataset[[col_prompt,col_response,col_reference]]\n",
    "    #print(eval_dataset_comp)\n",
    "    eval_dataset_comp = eval_dataset_comp.rename(columns={col_prompt: 'prompt', col_response: 'response', col_reference: 'reference'})\n",
    "    #print(eval_dataset_comp)\n",
    "    eval_task = EvalTask(\n",
    "        dataset=eval_dataset_comp, \n",
    "        metrics=[\"rouge_l_sum\",MetricPromptTemplateExamples.Pointwise.SUMMARIZATION_QUALITY, pointwise_custom_summary_metric],\n",
    "        )\n",
    "    eval_result = eval_task.evaluate().summary_metrics\n",
    "    eval_result_df = pd.DataFrame(eval_result, index=[col_response]).rename_axis('model').reset_index()\n",
    "    return eval_result_df\n",
    "\n",
    "# Evaluate different models\n",
    "results = [\n",
    "run_eval(df_final, \"dialogue\", \"gemini_response\", \"summary\"),\n",
    "run_eval(df_final, \"dialogue\", \"gemini_tuned_response\", \"summary\"),\n",
    "run_eval(df_final, \"dialogue\", \"summary_response\", \"summary\"),\n",
    "run_eval(df_final, \"dialogue\", \"openai_response\", \"summary\"),\n",
    "run_eval(df_final, \"dialogue\", \"openai_tuned_response\", \"summary\"),\n",
    "]\n",
    "\n",
    "# Combine results\n",
    "combined_comp_point_eval_result = pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5978f-5bc6-4146-871a-f4805211afcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_comp_point_eval_result\n",
    "#combined_comp_point_eval_result.to_csv('combined_comp_point_eval_result.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a3ab3-d9e5-4ca3-b9b5-90c394343282",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Pairwise (AutoSxS) Model Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a1ac5-5f71-4a38-8eb9-e3cbfb02055f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a pointwise custom summarization quality metric \n",
    "pairwise_custom_summary_metric_prompt = \"\"\"\n",
    "# Instruction\n",
    "You are an expert evaluator. Your task is to evaluate the quality of the responses generated by two AI models. We will provide you with the user input and a pair of AI-generated responses (Response A and Response B).\n",
    "You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n",
    "You will first judge responses individually, following the Rating Rubric and Evaluation Steps.\n",
    "Then you will give step-by-step explanations for your judgement, compare results to declare the winner based on the Rating Rubric and Evaluation Steps.\n",
    "# Evaluation\n",
    "## Metric Definition\n",
    "You will be assessing summarization quality, which measures the overall ability to summarize text.  The context to be summarized are provided in the user prompt. The response should be shorter than the text in the context. The response should not contain information that is not present in the context.\n",
    "\n",
    "## Criteria\n",
    "Less than 50 words: The response contains less than 50 words.  Use the following formula to count the words in the response: `=COUNTA(SPLIT(response, \" \"))`\n",
    "Groundedness: The response contains information included only in the context. The response does not reference any outside information.\n",
    "Observer Perspective: The response is written from an observer perspective.\n",
    "\n",
    "## Rating Rubric\n",
    "\"A\": Response A summarizes the given context as per the criteria better than response B.\n",
    "\"SAME\": Response A and B summarizes the given context equally well as per the criteria.\n",
    "\"B\": Response B summarizes the given context as per the criteria better than response A.\n",
    "\n",
    "## Evaluation Steps\n",
    "STEP 1: Analyze Response A based on the summarization quality criteria: Determine how well Response A fulfills the user requirements, is less than 50 words, is grounded and is written as an observer, and provide assessment according to the criterion.\n",
    "STEP 2: Analyze Response B based on the summarization quality criteria: Determine how well Response A fulfills the user requirements, is less than 50 words, is grounded and is written as an observer, and provide assessment according to the criterion.\n",
    "STEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment.\n",
    "STEP 4: Output your preference of \"A\", \"SAME\" or \"B\" to the pairwise_choice field according to the Rating Rubric.\n",
    "STEP 5: Output your assessment reasoning in the explanation field.\n",
    "\n",
    "\n",
    "# User Inputs and AI-generated Responses\n",
    "## User Inputs\n",
    "\n",
    "### Prompt\n",
    "{prompt}\n",
    "\n",
    "## AI-generated Responses\n",
    "### Response A\n",
    "{baseline_model_response}\n",
    "\n",
    "### Response B\n",
    "{response}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pairwise_custom_summary_metric = PairwiseMetric(\n",
    "  metric=\"custom_pairwise_summary_metric\",\n",
    "  metric_prompt_template=pairwise_custom_summary_metric_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed176375-c29e-4fff-9136-635036f3c868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataset_pair = df_final[['dialogue', 'gemini_tuned_response', 'openai_tuned_response']].rename(columns={\n",
    "    'dialogue': 'prompt', \n",
    "    'gemini_tuned_response': 'response', \n",
    "    'openai_tuned_response': 'baseline_model_response'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0678e-ce6e-4246-9809-56400edb9124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_task = EvalTask(\n",
    "    dataset=eval_dataset_pair, \n",
    "    metrics=[MetricPromptTemplateExamples.Pairwise.SUMMARIZATION_QUALITY, pairwise_custom_summary_metric],\n",
    "    )\n",
    "eval_result = eval_task.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80cee6-7548-47de-af2f-08d384a89b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_pair_eval_result=eval_result.summary_metrics\n",
    "#combined_pair_eval_result.to_csv('combined_pair_eval_result.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
